{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "import re\n",
    "from gensim.models import Word2Vec # 建立word2vec模型\n",
    "import pyltp\n",
    "from pyltp import SentenceSplitter\n",
    "from pyltp import Segmentor\n",
    "from pyltp import Postagger\n",
    "from pyltp import NamedEntityRecognizer\n",
    "from pyltp import Parser\n",
    "from pyltp import SementicRoleLabeller\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cut(string): return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\w\\d]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content = pd.read_csv('sqlResult_1558435.csv',encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content  = content.fillna('') # 去空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_content = ' '.join(content['content'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_content = ''.join(token(all_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\lenovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.766 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "all_cut_words = cut(all_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \n",
      "D:\\ProgramData\\Anaconda3\\Lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "mini_all_cut_words = all_cut_words[:10000]\n",
    "from gensim.models.word2vec import LineSentence\n",
    "with open('all_cut_words.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(' '.join(all_cut_words))\n",
    "news_word2vec = Word2Vec(LineSentence('all_cut_words.txt'), size=35, workers=8)\n",
    "keywords = news_word2vec.most_similar('说', topn=20)\n",
    "key_word_list = [i[0] for i in keywords]\n",
    "key_word_list.append('说')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = r'F:\\study\\NLP\\ltp_data_v3.4.0'\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR,  'cws.model')\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 依存句法分析模型路径，模型名称为`parser.model`\n",
    "srl_model_path = os.path.join(LTP_DATA_DIR, 'srl')  # 语义角色标注模型目录路径，模型目录为`srl`。注意该模型路径是一个目录，而不是一个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_splitter(sentence):\n",
    "#     sents = SentenceSplitter.split(sentence)\n",
    "    sents = sentence\n",
    "#     print('\\n'.join(sents))\n",
    "    sents_list = list(sents)\n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#分词   \n",
    "def segmentor(sentence):\n",
    "    segmentor =  Segmentor()\n",
    "    segmentor.load(cws_model_path)#加载模型\n",
    "    words = segmentor.segment(sentence) #分词\n",
    "    #默认可以这样输出\n",
    "#     print ('\\t'.join(words) + '\\n')\n",
    "    #可以转化成List输出\n",
    "    word_list = list(words)\n",
    "    segmentor.release()#释放模型\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#词性标注\n",
    "def posttagger(words):\n",
    "    postagger = Postagger()\n",
    "    postagger.load(pos_model_path)\n",
    "    posttags=postagger.postag(words)  #词性标注\n",
    "    postags = list(posttags)\n",
    "    postagger.release() #释放模型\n",
    "    #print type(postags)\n",
    "    return postags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#依存句法分析\n",
    "def par(words,postags):\n",
    "    parser = Parser() # 初始化实例\n",
    "    parser.load(par_model_path)  # 加载模型\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "#     for word,ntag in zip(words,netags):\n",
    "#         print(word+'/'+ ntag)\n",
    "#     print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs)\n",
    "    parser.release()  # 释放模型\n",
    "    return arcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 语义角色标注\n",
    "def sem(words, postags,arcs):\n",
    "    labeller = SementicRoleLabeller() # 初始化实例\n",
    "    labeller.load(srl_model_path)  # 加载模型\n",
    "    roles = labeller.label(words, postags,arcs)  # 语义角色标注\n",
    "    print('ok')\n",
    "    print(roles)\n",
    "    print ('*'*8)\n",
    "    \n",
    "#     for role in roles:\n",
    "#         print (role.index, \"\".join(\n",
    "#         [\"%s:(%d,%d)\" % (arg.name, arg.range.start, arg.range.end) for arg in role.arguments]))\n",
    "    labeller.release()  # 释放模型    \n",
    "    return roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#命名实体识别\n",
    "def ner(words,postags):\n",
    "    recognizer = NamedEntityRecognizer()\n",
    "    recognizer.load(ner_model_path) #加载模型\n",
    "    netags = recognizer.recognize(words,postags) #命名实体识别\n",
    "#     for word,ntag in zip(words,netags):\n",
    "#         print(word+'/'+ ntag)\n",
    "    recognizer.release()   #释放模型\n",
    "    nerttags = list(netags)\n",
    "    return nerttags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_news =\"\"\"刘德华坦言2019年可以再开一个世界巡回演唱会；元芳说想吃饺子；狄仁杰表示他也喜欢吃；黄飞鸿表示他今年还要拍一部电影；\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘德华\n",
      "2019年可以再开一个世界巡回演唱会\n",
      "\n",
      "********\n",
      "元芳\n",
      "想吃饺子\n",
      "\n",
      "********\n",
      "狄仁杰\n",
      "他也喜欢吃\n",
      "\n",
      "********\n",
      "黄飞鸿\n",
      "他今年还要拍一部电影\n",
      "\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "sents = SentenceSplitter.split(test_news)  # 分句\n",
    "for sent in sents:\n",
    "    words=segmentor(sent)\n",
    "    tags = posttagger(words)\n",
    "    nertags = ner(words,tags)\n",
    "    arcs = par(words, tags)\n",
    "    arcstags = list(arcs)\n",
    "    ans_arcs = [\"%d:%s\" % (arc.head, arc.relation) for arc in arcstags]\n",
    "    for word,nertag in zip(words,nertags):\n",
    "        if 'Nh' in nertag:\n",
    "            print(word)\n",
    "    for word,ans_arc in zip(words,ans_arcs):\n",
    "        if(ans_arc == '0:HED') & (word in key_word_list):\n",
    "            index_word = words.index(word)+1\n",
    "            print(\"\".join(arg for arg in words[index_word:-1]))\n",
    "    print('\\n'+ \"*\"*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
